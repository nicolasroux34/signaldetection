---
title: "Signal Detection"
author: "Nicolas Roux"
date: "14 April 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir= "C:/Users/Gast/Dropbox/Nico/Research/R/signal-detection")
```




The first chunk creates the basic data frame.
```{r,eval=FALSE}
library(rootSolve)
library(mvtnorm)
library(ggplot2)

df <- read.csv("data_sd.csv", header=TRUE,sep=";")
#rename some variables
names(df)[names(df)=="sujet"] <- "subject"
names(df)[names(df)=="groupe"] <- "group"
names(df)[names(df)=="accuracy_trial_indiv"] <- "success"
names(df)[names(df)=="conf_trial_indiv"] <- "confidence"
#Drop useless variables
df <- data.frame(df["subject"],df["trial"],df["success"],df["confidence"],df["expe"])
#Select data from expe 1
df <- df[df$expe==1,]
#Drop subject 20, which is the same as subject 18
df <- df[df$subject!=18,]
#Reset the subject counter
t <- df$subject >18
df$subject <- df$subject - t
#Drop the first 50 training observations for each individual
df <- df[df$trial>50,]
#Reset the trial counter
df$trial <- df$trial - 50
#Compute the calibration of each subject. positive calibration means the subject is overconfident.
mean.success <- sapply(split(df$success,df$subject), function(x) mean(x))
mean.confidence <- sapply(split(df$confidence,df$subject), function(x) mean(x))
calibration <- mean.success - mean.confidence
d <- 2*qnorm(mean.success)
#integrate those three variables in the data frame.
calibration <- rep(calibration,replicate(length(calibration),150))
mean.success <- rep(mean.success,replicate(length(mean.success),150))
mean.confidence <- rep(mean.confidence,replicate(length(mean.confidence),150))
df <- cbind(df,calibration,mean.confidence,mean.success)
df["calibrated.confidence"] <- df$confidence + df$calibration 
```

Function that predicts successes of a dyad using the confidence heuristic 

```{r,eval=FALSE}
confidence.heuristic <- function(x) {
  if (x$success[x$subject==i]==x$success[x$subject==j]) {
  s <- x$success[x$subject==i]
  return(s)
  } 
  else if (x$confidence[x$subject==i] > x$confidence[x$subject==j]) {
      s <- x$success[x$subject==i]
  } 
  else if (x$confidence[x$subject==i] < x$confidence[x$subject==j]) {
      s <- x$success[x$subject==j]
  }
  else if (x$confidence[x$subject==i] == x$confidence[x$subject==j]) {
      s <- rbinom(1, 1, .5)
  }
  return(s)
}

calibrated.confidence.heuristic <- function(x) {
  if (x$success[x$subject==i]==x$success[x$subject==j]) {
  s <- x$success[x$subject==i]
  return(s)
  } 
  else if (x$calibrated.confidence[x$subject==i] > x$calibrated.confidence[x$subject==j]) {
      s <- x$success[x$subject==i]
  } 
  else if (x$calibrated.confidence[x$subject==i] < x$calibrated.confidence[x$subject==j]) {
      s <- x$success[x$subject==j]
  }
  else if (x$calibrated.confidence[x$subject==i] == x$calibrated.confidence[x$subject==j]) {
      s <- rbinom(1, 1, .5)
  }
  return(s)
}
```

Function that is used to estimate the correlation between dyad members' perceptive signals if perceptive signals are drawn from a bivariate normal distribution. 
The function takes the difference between the theoretical probability of observing a joint success and its frequency.
```{r,eval=FALSE}

objective.correlation <- function(rho) {
  pmvnorm(lower=c(0,0), upper=c(Inf,Inf), mean=c(d.i/2,d.j/2),corr= rbind(c(1,rho),c(rho,1))) - mean.joint.success
}
```

This chunk assembles hypothetical dyads using our 66 individuals. The chunk creates a data frame that reports each dyad's input characteristics 
(individual detection indices, correlation, relative calibration...) and output characteristics (performance if the confidence heuristic is used, ideal detection index...).

```{r,eval=FALSE}
x <- seq(from = -.5, to = .8, by = .001)
data <- data.frame()
for (i in 1:64) {
  for (j in (i+1):65) {
    
    #computes the correlation.
    confidence.correlation <- cor(df$confidence[df$subject==i],df$confidence[df$subject==j])
    success.correlation <- cor(df$success[df$subject==i],df$success[df$subject==j])
    mean.success.ch <- mean(sapply(split(df,df$trial),function(x) confidence.heuristic(x))) 
    mean.success.cal.ch <- mean(sapply(split(df,df$trial),function(x) calibrated.confidence.heuristic(x))) 
    mean.success.i <- unique(df$mean.success[df$subject==i])
    d.i <- 2*qnorm(mean.success.i)
    mean.success.j <- unique(df$mean.success[df$subject==j])
    d.j <- 2*qnorm(mean.success.j)
    d.ch <- 2*qnorm(mean.success.ch)
    d.ch.calibrated <- 2*qnorm(mean.success.cal.ch)
    mean.joint.success <- mean(df$success[df$subject==i] == 1 & df$success[df$subject==j] == 1) 
    #Correlation parameter if individual signals are distributed following a normal bivariate.
    correlation <- x[which.max(-abs(sapply(x,function(x) objective.correlation(x))))] 
    
    
    d.min <- min(d.i,d.j)
    d.max <- max(d.i,d.j)
    equality <- d.min/d.max
    d.ideal <- (d.i^2+d.j^2-2*correlation*d.i*d.j)/((d.i-d.j*correlation)^2 + (d.j-d.i*correlation)^2 +2*correlation*(d.i-d.j*correlation)*(d.j-d.i*correlation))^(1/2)
    d.ideal.independent <- (d.i^2+d.j^2)/(d.i^2 + d.j^2)^(1/2)
    d.ch.theoretical <- (d.i^2+d.j^2)/(d.i^2 +d.j^2+2*correlation*d.i*d.j)^(1/2)
    efficiency <- d.ch/d.ideal 
    efficiency.theoretical <- d.ch.theoretical/d.ideal
    efficiency.calibrated <- d.ch.calibrated/d.ideal
    efficiency.independent <- d.ch/d.ideal.independent
    benefit <- d.ch/d.max
    benefit.theoretical <- d.ch.theoretical/d.max
    benefit.calibrated <- d.ch.calibrated/d.max
    benefit.ideal <- d.ideal/d.max
    
    
    data <- rbind(data, c(mean.joint.success,d.i,d.j,correlation,equality,d.max,d.ideal,d.ideal.independent,d.ch,d.ch.calibrated,d.ch.theoretical, efficiency,efficiency.theoretical,efficiency.calibrated,efficiency.independent,benefit,benefit.calibrated,benefit.theoretical))
    names(data) <- c("mean.joint.success","d.i","d.j","correlation","equality","d.max","d.ideal","d.ideal.independent","d.ch","d.ch.calibrated","d.ch.theoretical", "efficiency","efficiency.theoretical","efficiency.calibrated","efficiency.independent","benefit","benefit.calibrated","benefit.theoretical")
  }
}
save(data,file="data_ch.Rda")
```
#Data Analysis
```{r}
load("C:/Users/Gast/Dropbox/Nico/Research/R/signal-detection/data_ch.Rda")
data["dyad"] <- 1:2080
```
##Introduction



##Correlation Structure
I want to check whether the estimated correlation between perceptive signals is not systematically too high. The reason why I suspect it might is that the correlated version of the optimal model predicts a lower performance than that attained using the confidence heursitic for over 25% of groups. The correlation coefficient that are used in data_ch.Rda are computed based on the successes only, i.e. confidences are let out of the picture. I therefore compute the correlation between confidences that would arise if perceptive signals actually came from the standard detection model with the estimated coefficients. I want to check whether those theoretical correlation between confidences somehow match the observed correlation between confidences. 

The following code computes those theoretical correlations

```{r, eval=FALSE}
library('MASS')
theoretical.confidence.correlation.function <- function(x) {
  xixj <- mvrnorm(n = 1000, c(x$d.i/2,x$d.j/2), rbind(c(1,x$correlation),c(x$correlation,1)))
  ci <- pmax(dnorm(xixj[,1],x$d.i/2,1)/(dnorm(xixj[,1],x$d.i/2,1)+dnorm(xixj[,1],-x$d.i/2,1)),dnorm(xixj[,1],-x$d.i/2,1)/(dnorm(xixj[,1],x$d.i/2,1)+dnorm(xixj[,1],-x$d.i/2,1)))
  cj <- pmax(dnorm(xixj[,2],x$d.j/2,1)/(dnorm(xixj[,2],x$d.j/2,1)+dnorm(xixj[,2],-x$d.j/2,1)),dnorm(xixj[,2],-x$d.j/2,1)/(dnorm(xixj[,2],x$d.j/2,1)+dnorm(xixj[,2],-x$d.j/2,1)))
  cor(ci,cj)
}

theoretical.confidence.correlation <- sapply(split(data,data$dyad),function(x) theoretical.confidence.correlation.function(x))
data["theoretical.confidence.correlation"] <- theoretical.confidence.correlation

```

This following plots compares the distributions of theoretical and observed confidence correlation. They are remarkably similar, so there does not seem to be any systematic exageration of the correlation coefficients. 

However, the theoretical confidence does not predict the observed confidence correlation. 



## The Confidence Heuristic
In theory there are three factors which affect the relative performance of the confidence heuristic. 

*The first the calibration of confidences. When a participant is relatively overconfident relative to another, the confidence heuristic will follow the overconfident participant's too often. 

* The other two factors are the correlation of perceptive signals and the heterogeneity of detection indices, and work jointly. In a dyad where there is no correlation, or identical detection indices, the confidence heuristic will perform closely to the benchmark. On the contrary, when there are both correlated signals and heterogeneous detection indices, then the confidence heuristic will underperform. 

We are interested in the last two factors so we first attempt to neutralize the impact of confidence miscalibration. We do so by recalibrating the confidences of each participant. We do so in a rather naive way, i.e. our calibrated confidences are obtained by substracting the mean calibration error from the original confidence. This technique assumes that participants are uniformly over or underconfident, which is ntot quite true. Nevetheless, the recalibration of confidence measurably improves the performance of the confidence heuristic.


#Efficiency of the confidence heuristic and detection inequality. 
In all of this paper, we will refer to the confidence heuristic's peformance on a dyad as the dyad's peformance. 

A dyad is said __unequal__ when its members have unequal sensivity. More detection inequality means that participants' detection indices differ more from one another. A dyad is __efficient__ when is performs close to its ideal performance level, as predicted within a signal detection model. We measure efficient as the rattio of the confidence heuristic detection index over the ideal detection index.

A dyad is __miscalibrated__ when one participant is relatively overconfident, as compared to his teammate. A dyad may be well calibrated even if its members are not. Two participants which are equally underconfident will form a well calibrated dyad. The point is that one must be less underconfident than the other. 



Graphs C and D reproduce the analysis from Bahrami et al. (2010). They represent the benefit of communication, i.e. how much did dyad improve relative to its more sensitive member. Or put another way, to what extent did the more sensitive member benefit from the input of the least sensitive member. 

```{r}
library(ggplot2)
library(cowplot)
data$pred <- predict(lm(benefit ~ equality + equality^2, data = data))
data$ones <- rep(1,length(data$pred))

p1 <- ggplot(data, aes(x = equality, y = benefit))+ geom_point() +
  geom_line(aes(y = pred),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1)

data$pred1 <- predict(lm(benefit.calibrated ~ equality + equality^2, data = data))
data$ones <- rep(1,length(data$pred1))

p2 <- ggplot(data, aes(x = equality, y = benefit.calibrated)) + geom_point() +
  geom_line(aes(y = pred1),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1)

plot_grid(p1, p2, labels = c('C', 'D'),ncol=2)
```





The following graphs plots the relative detection improvement over $d_{max}$ predicted by the use of the confidence heuristic as well as the optimal integration of signals. 

```{r}
library(ggplot2)

fun.b.ch <- function(e,rho) ( 1+e^2 ) / ( 1+e^2+2*rho*e )^(1/2)
fun.b.opt <- function(e,rho) ( 1+e^2-2*rho*e ) / ( (1-rho*e)^2 + (e-rho)^2 + 2*rho*(e-rho)*(1-rho*e) )^(.5)

p <- ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x))
p + stat_function(fun = fun.b.ch,args = list(0.3),colour = "red") + stat_function(fun = fun.b.opt,args = list(0.3)) + stat_function(fun=function(x) 1,color="blue")

data$b.opt <- data$d.ideal/data$d.max
data$b.ch <- data$d.ch.theoretical/data$d.max

data.cor <- subset(data,correlation>0.55)

ggplot(data.cor, aes(y=b.opt,x=equality,colour = correlation)) + geom_point() +ylim(.9,1.6)
ggplot(data.cor, aes(y=b.ch,x=equality,color=correlation)) + geom_point() +ylim(.9,1.6)
ggplot(data.cor,aes(y=benefit.calibrated,x=equality,color=correlation))+ geom_point() 
p + stat_function(fun = fun.b.ch,args = list(0.6),colour = "red") + stat_function(fun = fun.b.opt,args = list(0.6)) + stat_function(fun=function(x) 1,color="blue")



```

Here, the value of s.max is 1 so that the s.ch and s.ideal equal the b.ch and b.ideal respectively. 
```{r}
library(ggplot2)
s.ch <- function(s.min, s.max,s) (s.max^2*sqrt(s.min^2+s^2)+s.min^2*sqrt(s.max^2+s^2)) / (s.max*(2*s.max^2*s.min^2+s^2*(s.max^2+s.min^2))^(.5))
s.ideal <- function(s.min,s.max) (s.max^2 + s.min^2)^(.5)


p <- ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x))
p + stat_function(fun = s.ch,args = list(1,0),colour = "red") + stat_function(fun = s.ch,args = list(1,0.2),colour = "blue") + stat_function(fun = s.ideal,args = list(1),colour = "black",size=1)

```
 
 Expressing our result using the s instead of d
 
 
```{r}
library(ggplot2)
b.ideal <- function(e,rho) (1+e^2)*(1+e^2+2*rho*e)^(-.5)
s.ideal <- function(s.i,s.j,rho) (s.i^2+s.j^2)*(s.i^2+s.j^2+2*rho*s.i*s.j)^(-.5)
p <- ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x))
p + stat_function(fun = b.ideal,args = list(0.3),colour = "red")
p + stat_function(fun= s.ideal, args=list(1,.5))
```

I take a group with maximal correlation, i.e. the joint success rate equals the success rate of the least sensitive participant. 


#Analysis a la Bahrami et al. (2010)
These first two plots show the inequality-efficiency relationship. We plot the benefits of collaboration, i.e. the ratio of the dyad's detection index (obtained through the confidence heuristic) and the detection index of its most sensitive member. 
```{r}
library(ggplot2)
library(cowplot)
data$equality2 <- data$equality^2
data$pred <- predict(lm(benefit ~ equality + equality2, data = data))
data$ones <- rep(1,length(data$pred))

p1 <- ggplot(data, aes(x = equality, y = benefit))+ geom_point() +
  geom_line(aes(y = pred),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1)

data$pred1 <- predict(lm(benefit.calibrated ~ equality + equality2, data = data))
data$ones <- rep(1,length(data$pred1))

p2 <- ggplot(data, aes(x = equality, y = benefit.calibrated)) + geom_point() +
  geom_line(aes(y = pred1),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1)

plot_grid(p1, p2, labels = c('C', 'D'),ncol=2)
```

```{r}
library(ggplot2)
library(cowplot)
data$equality2 <- data$equality^2
data.independent <- subset(data, data$correlation<0.22)

data.independent$pred <- predict(lm(benefit.calibrated ~ equality + equality2, data = data.independent))
data.independent$ones <- rep(1,length(data.independent$pred))

p1 <- ggplot(data.independent, aes(x = equality, y = benefit.calibrated))+ geom_point() +
  geom_line(aes(y = pred),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1)+ xlim(.2, 1) +ylim(.5,2)

data.correlated <- subset(data, data$correlation>0.22)

data.correlated$pred1 <- predict(lm(benefit.calibrated ~ equality + equality2, data = data.correlated))
data.correlated$ones <- rep(1,length(data.correlated$pred1))

p2 <- ggplot(data.correlated, aes(x = equality, y = benefit.calibrated)) + geom_point() +
  geom_line(aes(y = pred1),size=1,colour='red') + geom_line(aes(y=ones),color='blue',size = 1) + xlim(.2, 1)+ylim(.5,2)

plot_grid(p1, p2, labels = c('C', 'D'),ncol=2)
```

```{r}
ggplot(data,aes(x=equality,y=correlation)) + geom_point()
```


```{r}
hist(data$correlation)
var(data$correlation)
mean(data$correlation)
```

```{r}
hist(data$mean.joint.success)
```

The following curves gives a prediction on the relationship between equality and benefit that should be observed under the confidence heuristic. Assumption: (1) the distribution of correlation is normal, which is checked above. (2) equality and correlation are independent.

This line is the average benefit across dyads of a given equality, where the correlation parameters varies according to a normal distribution (the same for all equality levels).

```{r}
mean.success.i <- 0.84
mean.success.j <- 0.69
sigma.i <- 1/qnorm(mean.success.i)
sigma.j <- 1/qnorm(mean.success.j)
e <- sigma.i/sigma.j
mean.joint.success <- 0.5
objective.correlation <- function(rho) {
  pmvnorm(lower=c(0,0), upper=c(Inf,Inf), mean=c(1,1),sigma= rbind(c(sigma.i^2,rho*sigma.i*sigma.j),c(rho*sigma.i*sigma.j,sigma.j^2))) - mean.joint.success
}

x <- seq(from = -1, to = 1, by = .001)
correlation <- x[which.max(-abs(sapply(x,function(x) objective.correlation(x))))]
correlation
```
```{r}
sigma.i <- 1
sigma.j <- 2
rho <- .5
pmvnorm(lower=c(0,0), upper=c(Inf,Inf), mean=c(1,1),sigma= rbind(c(sigma.i^2,rho*sigma.i*sigma.j),c(rho*sigma.i*sigma.j,sigma.j^2)))
```

#The ideal s 
```{r}


s.ideal <- function(s.j,s.i,rho) {
  k.i <- s.i^2 - rho*s.i*s.j
  k.j <- s.j^2 - rho*s.i*s.j
  (k.i+k.j)*( k.i^2/s.i^2 + k.j^2/s.j^2 +(2*rho*k.i*k.j)/(s.i*s.j)  )^(-.5)
}
p <- ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x))
p + stat_function(fun= s.ideal, args=list(1,0.5))

```

